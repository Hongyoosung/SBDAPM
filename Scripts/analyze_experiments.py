"""
Experiment Analysis & Visualization Script

Reads CSV files generated by ExperimentTracker and produces:
- Win rate plots
- K/D ratio comparison charts
- MCTS performance metrics
- Coordination metrics
- Training convergence curves

Usage:
    python analyze_experiments.py --csv Saved/Experiments/SelfPlay_Training_*.csv --output Results/
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import argparse
import glob
import os
from pathlib import Path
from typing import List, Dict, Tuple

# Set plot style
sns.set_style("whitegrid")
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (12, 6)
plt.rcParams['figure.dpi'] = 150


class ExperimentAnalyzer:
    """Analyzes and visualizes SBDAPM experiment results"""

    def __init__(self, csv_files: List[str], output_dir: str = "Results"):
        """
        Initialize analyzer

        Args:
            csv_files: List of CSV file paths to analyze
            output_dir: Output directory for plots
        """
        self.csv_files = csv_files
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # Load all data
        print(f"Loading {len(csv_files)} CSV files...")
        self.df = self._load_data()
        print(f"Loaded {len(self.df)} episodes")

    def _load_data(self) -> pd.DataFrame:
        """Load and concatenate all CSV files"""
        dfs = []
        for csv_file in self.csv_files:
            try:
                df = pd.read_csv(csv_file)
                df['SourceFile'] = Path(csv_file).stem
                dfs.append(df)
            except Exception as e:
                print(f"Warning: Failed to load {csv_file}: {e}")

        if not dfs:
            raise ValueError("No CSV files loaded successfully")

        return pd.concat(dfs, ignore_index=True)

    def plot_win_rates(self, rolling_window: int = 10):
        """
        Plot win rates over time with rolling average

        Args:
            rolling_window: Window size for rolling average
        """
        print("Generating win rate plot...")

        # Calculate win rates
        df = self.df.copy()
        df['AlphaWin'] = (df['Winner'] == 'TeamAlpha').astype(int)
        df['BravoWin'] = (df['Winner'] == 'TeamBravo').astype(int)
        df['Draw'] = (df['Winner'] == 'Draw').astype(int)

        # Rolling averages
        df['AlphaWinRate'] = df['AlphaWin'].rolling(window=rolling_window, min_periods=1).mean() * 100
        df['BravoWinRate'] = df['BravoWin'].rolling(window=rolling_window, min_periods=1).mean() * 100

        # Plot
        fig, ax = plt.subplots(figsize=(14, 7))

        ax.plot(df['EpisodeNumber'], df['AlphaWinRate'], label='Team Alpha', linewidth=2, alpha=0.8)
        ax.plot(df['EpisodeNumber'], df['BravoWinRate'], label='Team Bravo', linewidth=2, alpha=0.8)
        ax.axhline(y=50, color='gray', linestyle='--', linewidth=1, alpha=0.5, label='50% (Balanced)')

        ax.set_xlabel('Episode Number', fontsize=12)
        ax.set_ylabel('Win Rate (%)', fontsize=12)
        ax.set_title(f'Win Rates Over Time (Rolling Avg: {rolling_window} episodes)', fontsize=14, fontweight='bold')
        ax.legend(fontsize=11)
        ax.grid(True, alpha=0.3)
        ax.set_ylim([0, 100])

        # Add final statistics box
        final_alpha_wr = df['AlphaWinRate'].iloc[-1]
        final_bravo_wr = df['BravoWinRate'].iloc[-1]
        total_episodes = len(df)

        stats_text = f"Final Win Rates:\nAlpha: {final_alpha_wr:.1f}%\nBravo: {final_bravo_wr:.1f}%\nTotal: {total_episodes} eps"
        ax.text(0.02, 0.98, stats_text, transform=ax.transAxes,
                fontsize=10, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

        plt.tight_layout()
        output_path = self.output_dir / 'win_rates.png'
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"✓ Saved: {output_path}")
        plt.close()

    def plot_kd_comparison(self):
        """Plot K/D ratio comparison between teams"""
        print("Generating K/D comparison plot...")

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

        # K/D over time
        ax1.plot(self.df['EpisodeNumber'], self.df['Alpha_KD'], label='Team Alpha', linewidth=2, alpha=0.7)
        ax1.plot(self.df['EpisodeNumber'], self.df['Bravo_KD'], label='Team Bravo', linewidth=2, alpha=0.7)
        ax1.axhline(y=1.0, color='gray', linestyle='--', linewidth=1, alpha=0.5, label='1.0 (Even)')

        ax1.set_xlabel('Episode Number', fontsize=12)
        ax1.set_ylabel('Kill/Death Ratio', fontsize=12)
        ax1.set_title('K/D Ratio Over Time', fontsize=14, fontweight='bold')
        ax1.legend(fontsize=11)
        ax1.grid(True, alpha=0.3)

        # K/D distribution (box plot)
        kd_data = [self.df['Alpha_KD'].dropna(), self.df['Bravo_KD'].dropna()]
        bp = ax2.boxplot(kd_data, labels=['Team Alpha', 'Team Bravo'],
                         patch_artist=True, showmeans=True)

        # Color boxes
        colors = ['#3498db', '#e74c3c']
        for patch, color in zip(bp['boxes'], colors):
            patch.set_facecolor(color)
            patch.set_alpha(0.6)

        ax2.set_ylabel('Kill/Death Ratio', fontsize=12)
        ax2.set_title('K/D Ratio Distribution', fontsize=14, fontweight='bold')
        ax2.grid(True, alpha=0.3, axis='y')
        ax2.axhline(y=1.0, color='gray', linestyle='--', linewidth=1, alpha=0.5)

        # Add mean values
        alpha_mean_kd = self.df['Alpha_KD'].mean()
        bravo_mean_kd = self.df['Bravo_KD'].mean()
        ax2.text(1, alpha_mean_kd, f'{alpha_mean_kd:.2f}', ha='center', va='bottom', fontsize=10, fontweight='bold')
        ax2.text(2, bravo_mean_kd, f'{bravo_mean_kd:.2f}', ha='center', va='bottom', fontsize=10, fontweight='bold')

        plt.tight_layout()
        output_path = self.output_dir / 'kd_comparison.png'
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"✓ Saved: {output_path}")
        plt.close()

    def plot_coordination_metrics(self):
        """Plot coordination metrics (combined fire, coordinated kills, formation)"""
        print("Generating coordination metrics plot...")

        fig, axes = plt.subplots(2, 2, figsize=(16, 12))

        # Coordinated Kills Rate
        ax = axes[0, 0]
        self.df['Alpha_CoordRate'] = (self.df['Alpha_CoordKills'] / self.df['Alpha_Kills'].replace(0, 1)) * 100
        self.df['Bravo_CoordRate'] = (self.df['Bravo_CoordKills'] / self.df['Bravo_Kills'].replace(0, 1)) * 100

        ax.plot(self.df['EpisodeNumber'], self.df['Alpha_CoordRate'], label='Team Alpha', linewidth=2)
        ax.plot(self.df['EpisodeNumber'], self.df['Bravo_CoordRate'], label='Team Bravo', linewidth=2)
        ax.set_xlabel('Episode Number')
        ax.set_ylabel('Coordinated Kills (%)')
        ax.set_title('Coordination Rate Over Time', fontweight='bold')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # Combined Fire Events
        ax = axes[0, 1]
        ax.plot(self.df['EpisodeNumber'], self.df['Alpha_CombinedFire'], label='Team Alpha', linewidth=2)
        ax.plot(self.df['EpisodeNumber'], self.df['Bravo_CombinedFire'], label='Team Bravo', linewidth=2)
        ax.set_xlabel('Episode Number')
        ax.set_ylabel('Combined Fire Events')
        ax.set_title('Combined Fire Events Over Time', fontweight='bold')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # Formation Coherence
        ax = axes[1, 0]
        ax.plot(self.df['EpisodeNumber'], self.df['Alpha_FormationCoherence'], label='Team Alpha', linewidth=2)
        ax.plot(self.df['EpisodeNumber'], self.df['Bravo_FormationCoherence'], label='Team Bravo', linewidth=2)
        ax.set_xlabel('Episode Number')
        ax.set_ylabel('Formation Coherence (0-1)')
        ax.set_title('Formation Coherence Over Time', fontweight='bold')
        ax.legend()
        ax.grid(True, alpha=0.3)
        ax.set_ylim([0, 1])

        # Objectives Completed
        ax = axes[1, 1]
        width = 0.35
        x = np.arange(2)
        alpha_obj = [self.df['Alpha_ObjectivesComplete'].sum(), self.df['Alpha_ObjectivesFailed'].sum()]
        bravo_obj = [self.df['Bravo_ObjectivesComplete'].sum(), self.df['Bravo_ObjectivesFailed'].sum()]

        ax.bar(x - width/2, alpha_obj, width, label='Team Alpha', color='#3498db', alpha=0.7)
        ax.bar(x + width/2, bravo_obj, width, label='Team Bravo', color='#e74c3c', alpha=0.7)

        ax.set_ylabel('Count')
        ax.set_title('Total Objectives', fontweight='bold')
        ax.set_xticks(x)
        ax.set_xticklabels(['Completed', 'Failed'])
        ax.legend()
        ax.grid(True, alpha=0.3, axis='y')

        plt.tight_layout()
        output_path = self.output_dir / 'coordination_metrics.png'
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"✓ Saved: {output_path}")
        plt.close()

    def plot_mcts_performance(self):
        """Plot MCTS performance metrics (latency, variance, entropy)"""
        if 'Alpha_MCTSLatency' not in self.df.columns:
            print("⚠ MCTS metrics not found in data, skipping MCTS performance plot")
            return

        print("Generating MCTS performance plot...")

        fig, axes = plt.subplots(2, 2, figsize=(16, 12))

        # MCTS Latency
        ax = axes[0, 0]
        ax.plot(self.df['EpisodeNumber'], self.df['Alpha_MCTSLatency'], label='Team Alpha', linewidth=2)
        ax.plot(self.df['EpisodeNumber'], self.df['Bravo_MCTSLatency'], label='Team Bravo', linewidth=2)
        ax.axhline(y=50, color='red', linestyle='--', linewidth=1, alpha=0.5, label='Target (50ms)')
        ax.set_xlabel('Episode Number')
        ax.set_ylabel('Latency (ms)')
        ax.set_title('MCTS Execution Latency', fontweight='bold')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # Value Variance (Uncertainty)
        ax = axes[0, 1]
        ax.plot(self.df['EpisodeNumber'], self.df['Alpha_ValueVariance'], label='Team Alpha', linewidth=2)
        ax.plot(self.df['EpisodeNumber'], self.df['Bravo_ValueVariance'], label='Team Bravo', linewidth=2)
        ax.set_xlabel('Episode Number')
        ax.set_ylabel('Value Variance')
        ax.set_title('MCTS Value Uncertainty', fontweight='bold')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # Policy Entropy
        ax = axes[1, 0]
        ax.plot(self.df['EpisodeNumber'], self.df['Alpha_PolicyEntropy'], label='Team Alpha', linewidth=2)
        ax.plot(self.df['EpisodeNumber'], self.df['Bravo_PolicyEntropy'], label='Team Bravo', linewidth=2)
        ax.set_xlabel('Episode Number')
        ax.set_ylabel('Policy Entropy')
        ax.set_title('MCTS Policy Exploration', fontweight='bold')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # MCTS Executions per Episode
        ax = axes[1, 1]
        ax.plot(self.df['EpisodeNumber'], self.df['Alpha_MCTSExecutions'], label='Team Alpha', linewidth=2)
        ax.plot(self.df['EpisodeNumber'], self.df['Bravo_MCTSExecutions'], label='Team Bravo', linewidth=2)
        ax.set_xlabel('Episode Number')
        ax.set_ylabel('MCTS Executions')
        ax.set_title('MCTS Execution Frequency', fontweight='bold')
        ax.legend()
        ax.grid(True, alpha=0.3)

        plt.tight_layout()
        output_path = self.output_dir / 'mcts_performance.png'
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"✓ Saved: {output_path}")
        plt.close()

    def plot_reward_breakdown(self):
        """Plot hierarchical reward breakdown"""
        if 'Alpha_TotalReward' not in self.df.columns:
            print("⚠ Reward metrics not found in data, skipping reward breakdown plot")
            return

        print("Generating reward breakdown plot...")

        fig, axes = plt.subplots(2, 2, figsize=(16, 12))

        # Total Reward Over Time
        ax = axes[0, 0]
        ax.plot(self.df['EpisodeNumber'], self.df['Alpha_TotalReward'], label='Team Alpha', linewidth=2)
        ax.plot(self.df['EpisodeNumber'], self.df['Bravo_TotalReward'], label='Team Bravo', linewidth=2)
        ax.set_xlabel('Episode Number')
        ax.set_ylabel('Total Reward')
        ax.set_title('Total Reward Over Time', fontweight='bold')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # Reward Components - Team Alpha
        ax = axes[0, 1]
        alpha_rewards = [
            self.df['Alpha_IndividualReward'].sum(),
            self.df['Alpha_CoordinationReward'].sum(),
            self.df['Alpha_StrategicReward'].sum()
        ]
        ax.bar(['Individual', 'Coordination', 'Strategic'], alpha_rewards, color='#3498db', alpha=0.7)
        ax.set_ylabel('Cumulative Reward')
        ax.set_title('Team Alpha - Reward Breakdown', fontweight='bold')
        ax.grid(True, alpha=0.3, axis='y')

        # Reward Components - Team Bravo
        ax = axes[1, 0]
        bravo_rewards = [
            self.df['Bravo_IndividualReward'].sum(),
            self.df['Bravo_CoordinationReward'].sum(),
            self.df['Bravo_StrategicReward'].sum()
        ]
        ax.bar(['Individual', 'Coordination', 'Strategic'], bravo_rewards, color='#e74c3c', alpha=0.7)
        ax.set_ylabel('Cumulative Reward')
        ax.set_title('Team Bravo - Reward Breakdown', fontweight='bold')
        ax.grid(True, alpha=0.3, axis='y')

        # Reward Proportions (Pie Chart)
        ax = axes[1, 1]
        total_alpha = sum(alpha_rewards)
        total_bravo = sum(bravo_rewards)
        combined_rewards = [alpha_rewards[i] + bravo_rewards[i] for i in range(3)]

        colors = ['#3498db', '#2ecc71', '#f39c12']
        wedges, texts, autotexts = ax.pie(combined_rewards,
                                            labels=['Individual', 'Coordination', 'Strategic'],
                                            autopct='%1.1f%%',
                                            colors=colors,
                                            startangle=90)
        ax.set_title('Combined Reward Distribution', fontweight='bold')

        # Make percentage text bold
        for autotext in autotexts:
            autotext.set_color('white')
            autotext.set_fontweight('bold')

        plt.tight_layout()
        output_path = self.output_dir / 'reward_breakdown.png'
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"✓ Saved: {output_path}")
        plt.close()

    def plot_episode_duration(self):
        """Plot episode duration over time"""
        print("Generating episode duration plot...")

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

        # Duration over time
        ax1.plot(self.df['EpisodeNumber'], self.df['Duration'], linewidth=2, color='#9b59b6')
        ax1.set_xlabel('Episode Number')
        ax1.set_ylabel('Duration (seconds)')
        ax1.set_title('Episode Duration Over Time', fontweight='bold')
        ax1.grid(True, alpha=0.3)

        # Duration histogram
        ax2.hist(self.df['Duration'], bins=30, color='#9b59b6', alpha=0.7, edgecolor='black')
        ax2.axvline(self.df['Duration'].mean(), color='red', linestyle='--', linewidth=2, label='Mean')
        ax2.axvline(self.df['Duration'].median(), color='green', linestyle='--', linewidth=2, label='Median')
        ax2.set_xlabel('Duration (seconds)')
        ax2.set_ylabel('Frequency')
        ax2.set_title('Episode Duration Distribution', fontweight='bold')
        ax2.legend()
        ax2.grid(True, alpha=0.3, axis='y')

        plt.tight_layout()
        output_path = self.output_dir / 'episode_duration.png'
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"✓ Saved: {output_path}")
        plt.close()

    def generate_summary_report(self):
        """Generate text summary report"""
        print("Generating summary report...")

        report = []
        report.append("=" * 80)
        report.append("SBDAPM EXPERIMENT ANALYSIS REPORT")
        report.append("=" * 80)
        report.append(f"\nTotal Episodes: {len(self.df)}")
        report.append(f"Experiment Files: {len(self.csv_files)}")

        # Win rates
        report.append("\n" + "-" * 80)
        report.append("WIN RATES")
        report.append("-" * 80)
        alpha_wins = (self.df['Winner'] == 'TeamAlpha').sum()
        bravo_wins = (self.df['Winner'] == 'TeamBravo').sum()
        draws = (self.df['Winner'] == 'Draw').sum()

        report.append(f"Team Alpha: {alpha_wins:>4} wins ({100*alpha_wins/len(self.df):>5.1f}%)")
        report.append(f"Team Bravo: {bravo_wins:>4} wins ({100*bravo_wins/len(self.df):>5.1f}%)")
        report.append(f"Draws:      {draws:>4} ({100*draws/len(self.df):>5.1f}%)")

        # Combat statistics
        report.append("\n" + "-" * 80)
        report.append("COMBAT STATISTICS")
        report.append("-" * 80)
        report.append(f"{'Metric':<25} {'Team Alpha':>15} {'Team Bravo':>15}")
        report.append("-" * 80)
        report.append(f"{'Avg K/D Ratio':<25} {self.df['Alpha_KD'].mean():>15.2f} {self.df['Bravo_KD'].mean():>15.2f}")
        report.append(f"{'Avg Kills/Episode':<25} {self.df['Alpha_Kills'].mean():>15.1f} {self.df['Bravo_Kills'].mean():>15.1f}")
        report.append(f"{'Avg Damage Dealt':<25} {self.df['Alpha_DamageDealt'].mean():>15.0f} {self.df['Bravo_DamageDealt'].mean():>15.0f}")
        report.append(f"{'Avg Damage Taken':<25} {self.df['Alpha_DamageTaken'].mean():>15.0f} {self.df['Bravo_DamageTaken'].mean():>15.0f}")

        # Coordination metrics
        report.append("\n" + "-" * 80)
        report.append("COORDINATION METRICS")
        report.append("-" * 80)
        alpha_coord_rate = 100 * self.df['Alpha_CoordKills'].sum() / max(self.df['Alpha_Kills'].sum(), 1)
        bravo_coord_rate = 100 * self.df['Bravo_CoordKills'].sum() / max(self.df['Bravo_Kills'].sum(), 1)

        report.append(f"{'Metric':<25} {'Team Alpha':>15} {'Team Bravo':>15}")
        report.append("-" * 80)
        report.append(f"{'Coordination Rate':<25} {alpha_coord_rate:>14.1f}% {bravo_coord_rate:>14.1f}%")
        report.append(f"{'Avg Combined Fire':<25} {self.df['Alpha_CombinedFire'].mean():>15.1f} {self.df['Bravo_CombinedFire'].mean():>15.1f}")
        report.append(f"{'Avg Formation Coh.':<25} {self.df['Alpha_FormationCoherence'].mean():>15.3f} {self.df['Bravo_FormationCoherence'].mean():>15.3f}")

        # Episode statistics
        report.append("\n" + "-" * 80)
        report.append("EPISODE STATISTICS")
        report.append("-" * 80)
        report.append(f"Average Duration:     {self.df['Duration'].mean():>6.1f} seconds")
        report.append(f"Median Duration:      {self.df['Duration'].median():>6.1f} seconds")
        report.append(f"Min Duration:         {self.df['Duration'].min():>6.1f} seconds")
        report.append(f"Max Duration:         {self.df['Duration'].max():>6.1f} seconds")

        if 'AvgFPS' in self.df.columns:
            report.append(f"\nAverage FPS:          {self.df['AvgFPS'].mean():>6.1f}")

        # MCTS performance
        if 'Alpha_MCTSLatency' in self.df.columns:
            report.append("\n" + "-" * 80)
            report.append("MCTS PERFORMANCE")
            report.append("-" * 80)
            report.append(f"{'Metric':<25} {'Team Alpha':>15} {'Team Bravo':>15}")
            report.append("-" * 80)
            report.append(f"{'Avg Latency (ms)':<25} {self.df['Alpha_MCTSLatency'].mean():>15.2f} {self.df['Bravo_MCTSLatency'].mean():>15.2f}")
            report.append(f"{'Max Latency (ms)':<25} {self.df['Alpha_MCTSLatency'].max():>15.2f} {self.df['Bravo_MCTSLatency'].max():>15.2f}")
            report.append(f"{'Avg Executions/Ep':<25} {self.df['Alpha_MCTSExecutions'].mean():>15.1f} {self.df['Bravo_MCTSExecutions'].mean():>15.1f}")

        report.append("\n" + "=" * 80)

        # Save report
        report_text = "\n".join(report)
        output_path = self.output_dir / 'summary_report.txt'
        with open(output_path, 'w') as f:
            f.write(report_text)

        print(f"✓ Saved: {output_path}")
        print("\n" + report_text)

    def generate_all_plots(self):
        """Generate all analysis plots"""
        print("\n" + "=" * 80)
        print("GENERATING ALL PLOTS")
        print("=" * 80 + "\n")

        self.plot_win_rates()
        self.plot_kd_comparison()
        self.plot_coordination_metrics()
        self.plot_mcts_performance()
        self.plot_reward_breakdown()
        self.plot_episode_duration()
        self.generate_summary_report()

        print("\n" + "=" * 80)
        print(f"✅ ALL PLOTS SAVED TO: {self.output_dir.absolute()}")
        print("=" * 80 + "\n")


def main():
    parser = argparse.ArgumentParser(description='Analyze SBDAPM experiment results')
    parser.add_argument('--csv', type=str, nargs='+',
                        help='CSV file(s) to analyze (supports wildcards)')
    parser.add_argument('--output', type=str, default='Results',
                        help='Output directory for plots (default: Results)')
    parser.add_argument('--rolling', type=int, default=10,
                        help='Rolling window size for win rate plot (default: 10)')

    args = parser.parse_args()

    # Expand wildcards
    csv_files = []
    if args.csv:
        for pattern in args.csv:
            csv_files.extend(glob.glob(pattern))

    if not csv_files:
        print("Error: No CSV files found")
        print("\nUsage examples:")
        print("  python analyze_experiments.py --csv Saved/Experiments/SelfPlay_*.csv")
        print("  python analyze_experiments.py --csv experiment1.csv experiment2.csv --output MyResults/")
        return

    # Analyze
    analyzer = ExperimentAnalyzer(csv_files, args.output)
    analyzer.generate_all_plots()


if __name__ == '__main__':
    main()
